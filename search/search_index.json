{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Efficient Power Level Exporter (Kepler)","text":"<p>Kepler (Kubernetes-based Efficient Power Level Exporter) is a Prometheus exporter. It uses eBPF to probe CPU performance counters and Linux kernel tracepoints.</p> <p>These data and stats from cgroup and sysfs can then be fed into ML models to estimate energy consumption by Pods.</p> <p>Check out the project on GitHub \u27a1\ufe0f Kepler.</p> <p></p> <p> We are a Cloud Native Computing Foundation sandbox project.   </p>"},{"location":"design/architecture/","title":"Components","text":""},{"location":"design/architecture/#kepler-exporter","title":"Kepler Exporter","text":"<p>Kepler Exporter exposes a variety of metrics about the energy consumption of Kubernetes components such as Pods and Nodes.</p> <p>Monitor container power consumption with the metrics made available by the Kepler Exporter.</p> <p></p>"},{"location":"design/architecture/#kepler-model-server","title":"Kepler Model Server","text":"<p>The main feature of Kepler Model Server is to return a power estimation model correponding to the request containing target granularity (node in total, node per each processor component, pod in total, pod per each processor component), available input metrics, model filters such as acccuracy.</p> <p>In addition, the online-trainer can be deployed as a sidecar container to the server (main container) to execute trainning pipelines and update the model on the fly when power metrics are available.</p> <p>Check us out on GitHub \u27a1\ufe0f Kepler Model Server</p>"},{"location":"design/architecture/#kepler-estimator-sidecar","title":"Kepler Estimator Sidecar","text":"<p>Kepler estimator is a client module to kepler model server running as a sidecar of Kepler exporter (main container).</p> <p>This python will serve a PowerReequest from model package in Kepler exporter as defined in estimator.go via unix domain socket <code>/tmp/estimator.sock</code>.</p> <p>Check us out on GitHub \u27a1\ufe0f Kepler Estimator</p>"},{"location":"design/metrics/","title":"Monitoring Container Power Consumption with Kepler","text":"<p>Kepler Exporter exposes statistics from an application running in a Kubernetes cluster in a Prometheus-friendly format that can be scraped by any database that understands this format, such as <code>Prometheus</code> and <code>Sysdig</code>.</p> <p>Kepler exports a variety of container metrics to Prometheus, where the main ones are those related  to energy consumption. </p>"},{"location":"design/metrics/#kepler-metrics-overview","title":"Kepler metrics overview","text":"<p>All the metrics specific to the Kepler Exporter are prefixed with <code>kepler_</code>.</p>"},{"location":"design/metrics/#kepler-metrics-for-container-energy-consumption","title":"Kepler metrics for Container Energy Consumption:","text":"<ul> <li> <p>kepler_container_joules_total (Counter)     This metric is the aggregated package/socket energy consumption of CPU, dram, gpus, and other host components for a given container.     Each component has individual metrics which are detailed next in this document.</p> <p>This metric simplifies the Prometheus metric for performance reasons. A very large promQL query typically introduces a very high overhead on Prometheus.</p> </li> <li> <p>kepler_container_core_joules_total (Counter)     This measures the total energy consumption on CPU cores that  a certain container has used.     Generally, when the system has access to <code>RAPL</code>_ metrics, this metric will reflect the porportinal container energy consumption of the RAPL     Power Plan 0 (PP0), which is the energy consumed by all CPU cores in the socket.     However, this metric is processor model specific and may not be available on some server CPUs.     The RAPL CPU metric that is available on all processors that support RAPL is the package, which we will detail     on another metric.</p> <p>In some cases where RAPL is available but core metrics are not, Kepler may use the energy consumption package. But note that package energy consumption is not just from CPU cores, it is all socket energy consumption.</p> <p>In case <code>RAPL</code>_ is not available, kepler might estimate this metric using the model server.</p> </li> <li> <p>kepler_container_dram_joules_total (Counter)     This metric describes the total energy spent in DRAM by a container.</p> </li> <li> <p>kepler_container_uncore_joules_total (Counter)     This measures the cumulative energy consumed by certain uncore components, which are typically the last level cache,     integrated GPU and memory controller, but the number of components may vary depending on the system.     The uncore metric is processor model specific and may not be available on some server CPUs.</p> <p>When <code>RAPL</code>_ is not available, kepler can estimate this metric using the model server if the node CPU supports the uncore metric.</p> </li> <li> <p>kepler_container_package_joules_total (Counter)     This measures the cumulative energy consumed by the CPU socket, including all cores and uncore components (e.g.     last-level cache, integrated GPU and memory controller).     RAPL package energy is typically the PP0 + PP1, but PP1 counter may or may not account for all energy usage     by uncore components. Therefore, package energy consumption may be higher than core + uncore.</p> <p>When <code>RAPL</code>_ is not available, kepler might estimate this metric using the model server.</p> </li> <li> <p>kepler_container_other_host_components_joules_total (Counter)     This measures the cumulative energy consumption on other host components besides the CPU and DRAM.     The vast majority of motherboards have a energy consumption sensor that can be accessed via the kernel acpi or ipmi.     This sensor reports the energy consumption of the entire system.     In addition, some processor architectures support the RAPL platform domain (PSys) which is the energy consumed by the     \"System on a chipt\" (SOC).</p> <p>Generally, this metric is the host energy consumption (from acpi) less the RAPL Package and DRAM.</p> </li> <li> <p>kepler_container_gpu_joules_total (Counter)     This measures the total energy consumption on the GPUs that  a certain container has used.       Currently, Kepler only supports NVIDIA GPUs, but this metric will also reflect other accelerators in the future.       So when the system has NVIDIA GPUs, kepler can calculate the energy consumption of the container's gpu using the GPU's       processeses energy consumption and utilization via NVIDIA nvml package.</p> </li> <li> <p>kepler_container_energy_stat (Counter)     This metric contains several container metrics labeled with container resource utilization cgroup metrics     that are used in the model server for predictions.</p> <p>This metric is specific for the model server and might be updated any time.</p> </li> </ul> <p>Note:     \"system_process\" is a special indicator that aggregate all the non-container workload into system process consumption metric.</p>"},{"location":"design/metrics/#kepler-metrics-for-container-resource-utilization","title":"Kepler metrics for Container resource utilization:","text":"<ul> <li> <p>kepler_container_cpu_cycles_total (Counter)     This measures the total CPU cycles used by the container using hardware counters.     To support fine-grained analysis of performance and resource utilization, hardware counters are particularly desirable     due to its granularity and precision..</p> <p>The CPU cycles is a metric directly related to CPU frequency. On systems where processors run at a fixed frequency, CPU cycles and total CPU time are roughly equivalent. On systems where processors run at varying frequencies, CPU cycles and total CPU time will have different values.</p> </li> <li> <p>kepler_container_cpu_instructions_total (Counter)     This measure the total cpu instructions used by the container using hardware counters.</p> <p>CPU instructions are the de facto metric for accounting for CPU utilization.</p> </li> <li> <p>kepler_container_cache_miss_total (Counter)     This measures the total cache miss that has occurred for a given container using hardware counters.</p> <p>As there is no event counter that measures memory access directly, the number of last-level cache misses gives a good proxy for the memory access number. If an LLC read miss occurs, a read access to main memory should occur (but note that this is not necessarily the case for LLC write misses under a write-back cache policy).</p> </li> </ul> <p>Note:     You can enable/disable expose of those metrics through <code>expose-hardware-counter-metrics</code> kepler execution option.</p>"},{"location":"design/metrics/#kepler-metrics-for-node-information","title":"Kepler metrics for Node information:","text":"<ul> <li> <p>kepler_node_nodeInfo (Counter)     This metric shows the node metada like the node CPU architecture.</p> <p>Note that this metrics is deprecated and might be updated to <code>kepler_node_info</code> in the next release.</p> </li> </ul>"},{"location":"design/metrics/#kepler-metrics-for-node-energy-consumption","title":"Kepler metrics for Node energy consumption:","text":"<ul> <li> <p>kepler_node_core_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_uncore_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_dram_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_package_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_other_host_components_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_gpu_joules_total (Counter)     Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_platform_joules_total (Counter)    This metric represents the total energy consumption of the host.</p> <p>The vast majority of motherboards have a energy consumption sensor that can be accessed via the acpi or ipmi kernel. This sensor reports the energy consumption of the entire system. In addition, some processor architectures support the RAPL platform domain (PSys) which is the energy consumed by the \"System on a chipt\" (SOC).</p> <p>Generally, this metric is the host energy consumption (from acpi).</p> </li> <li> <p>kepler_node_energy_stat (Counter)     This metric contains multiple metrics from nodes labeled with container resource utilization cgroup metrics     that are used in the model server.</p> <p>This metric is specific to the model server and can be updated at any time.</p> </li> </ul>"},{"location":"design/metrics/#exploring-node-exporter-metrics-through-the-prometheus-expression","title":"Exploring Node Exporter metrics through the Prometheus expression","text":"<p>All the energy consumption metrics are defined as counter following the <code>Prometheus metrics guide &lt;https://prometheus.io/docs/practices/naming/&gt;</code>_ for energy related metrics.</p> <p>The <code>rate()</code> of joules gives the power in Watts since the rate function returns the average per second. Therefore, for get the container energy consumption you can use the following query:</p> <p><code>sum by (pod_name, container_name, container_namespace, node) (</code> <code>irate(kepler_container_joules_total{}[1m])</code> <code>)</code></p> <p>Note that we report the node label in the container metrics because the OS metrics \"system_process\" will have the same name and namespace across all nodes and we do not want to aggregate them.</p>"},{"location":"design/metrics/#rapl-power-domain","title":"RAPL power domain","text":"<p><code>RAPL power domains supported &lt;https://zhenkai-zhang.github.io/papers/rapl.pdf&gt;</code>_ in some  resent Intel microarchitecture (consumer-grade/server-grade):</p> Microarchitecture Package CORE (PP0) UNCORE (PP1) DRAM Haswell Y/Y Y/N Y/N Y/Y Broadwell Y/Y Y/N Y/N Y/Y Skylake Y/Y Y/Y Y/N Y/Y Kaby Lake Y/Y Y/Y Y/N Y/Y <p>.. _Prometheus: https://prometheus.io</p> <p>.. _Sysdig: https://sysdig.com/</p> <p>.. _RAPL: https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/advisory-guidance/running-average-power-limit-energy-reporting.html</p>"},{"location":"design/power_estimation/","title":"Kepler Power Estimation","text":"<p>In Kepler, we also provide a power estimation solution from the resource usages in the system that there is no power measuring tool installed or supported.  There are two alternatives of estimators.</p>"},{"location":"design/power_estimation/#estimators","title":"Estimators","text":"<ul> <li> <p>Local Linear Regression Estimator: This estimator estimates power using the trained weights multiplied by normalized value of usage metrics (Linear Regression Model).</p> </li> <li> <p>General Estimator Sidecar: This estimator transforms the usage metrics and applies with the trained models which can be any regression models from scikit-learn library or any neuron networks from Keras (TensorFlow). To use this estimator, the Kepler General Estimator component needs to be enabled.</p> </li> </ul> <p>On top of that, the trained models as well as weights can be updated periodically with online trainning routine by connecting the Kepler Model Server component.</p>"},{"location":"design/power_estimation/#deployment-scenarios","title":"Deployment Scenarios","text":"<p>Minimum Deployment</p> <p>The minimum deployment is to use local linear regression estimator in Kepler main container with only offline-trained model weights. </p> <p></p> <p>Deployment with General Estimator Sidecar</p> <p>To enable general estimator for power inference, the estimator sidecar can be deployed as shown in the following figure.  The connection between two containers is a unix domain socket which is lightweight and fast. Unlike the local estimator, the general estimator sidecar is instrumented with several inference-supportive libraries and dependencies. This additional overhead must be tradeoff to an increasing estimation accuracy expected from flexible choices of models.</p> <p></p> <p>Minimum deployment connecting to Kepler Model Server</p> <p>To get the updated weights which is expected to provide better estimation accuracy, Kepler may connect to remote Kepler Model Server that performs online trainning using data from the system with the power measuring tool as below.</p> <p></p> <p>Full deployment</p> <p>The following figure shows the deployment that Kepler General Estimator is enabled and it is also connecting to remote Kepler Model Server.  The Kepler General Estimator sidecar can update the model from the Kepler Model Server on the fly and expect the most accurate model.</p> <p></p>"},{"location":"design/power_model/","title":"Kepler Power Model","text":"<p>In Kepler, with respective to available measurements, we provide a pod-level power with a mix of two power modeling approaches:</p>"},{"location":"design/power_model/#modeling-approach","title":"Modeling Approach","text":"<ul> <li> <p>Power Ratio Modeling: This modeling computes a finer-grained power by the usage ratio over the total summation of power. This modeling is used by default when the total power is known.</p> </li> <li> <p>Power Estimation Modeling: This modeling estimates a power by using usage metrics as input features of the trained model. This modeling can be used even if the power metric cannot be measured. The estimation can be done in three levels: Node total power (including fan, power supply, etc.), Node internal component powers (such as CPU, Memory), Pod power. </p> <p>also see Kepler Power Estimation.</p> </li> </ul>"},{"location":"design/power_model/#usage-scenario","title":"Usage Scenario","text":"Scenario Node Total Power Node Component Powers Pod Power BM (x86 with power meter) Measurement (e.g., ACPI) Measurement (RAPL) Power Ratio BM (x86 but no power meter) Power Estimation Measurement Power Ratio BM (non-x86 with power meter) Measurement Power Estimation Power Ratio BM (non-x86 and no power meter) Power Estimation Power Estimation Power Ratio VM with node info and power passthrough from BM (x86 with power meter) Measurement + VM Mapping Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (x86 but no power meter) Power Estimation Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (non-x86 with power meter) Measurement + VM Mapping Power Estimation Power Ratio VM with node info Power Estimation Power Estimation Power Ratio Pure VM - - Power Estimation"},{"location":"installation/kepler/","title":"Kepler Installation","text":""},{"location":"installation/kepler/#requirements","title":"Requirements","text":"<ul> <li>Kernel 4.18+</li> <li>Access to a Kubernetes cluster</li> <li><code>kubectl</code> v1.21.0+</li> </ul>"},{"location":"installation/kepler/#deployments","title":"Deployments","text":""},{"location":"installation/kepler/#deploy-using-helm-chart","title":"Deploy using Helm Chart","text":"<p>The Kepler Helm Chart is available on GitHub and ArtifactHub </p> <p>For Installation Helm must be installed to use the charts. Please refer to Helm's documentation to get started.</p> <p>The chart is accessible using the following commands:</p> <p>Add the helm repo</p> <pre><code>helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart\n</code></pre> <p>You can see the latest version by using the folllowing command:</p> <pre><code>helm search repo kepler\n</code></pre> <p>If you would like to test and look at the manifest files before deploying you can run:</p> <pre><code>helm install kepler kepler/kepler --namespace kepler --create-namespace --dry-run --devel\n</code></pre> <p>Then to install run the following:</p> <pre><code>helm install kepler kepler/kepler --namespace kepler --create-namespace\n</code></pre> <p>You may want to override values.yaml file use the following command.</p> <pre><code>helm install kepler kepler/kepler --values values.yaml --namespace kepler --create-namespace\n</code></pre> <p>The following table lists the configurable parameters for this chart and their default values.</p> Parameter Description Default global.namespace Kubernete namespace for kepler kepler image.repository Repository for Kepler Image quay.io/sustainable_computing_io/kepler image.pullPolicy Pull policy for Kepler Always image.tag Image tag for Kepler Image latest serviceAccount.name Service acccount name for Kepler kepler-sa service.type Kepler service type ClusterIP service.port Kepler service exposed port 9102"},{"location":"installation/kepler/#uninstall-the-kepler","title":"Uninstall the kepler","text":"<p>To uninstall this chart, use the following steps</p> <pre><code>helm delete --purge kepler --tiller-namespace &lt;namespace&gt;\n</code></pre>"},{"location":"installation/kepler/#deploy-from-source-code","title":"Deploy from source code","text":"<p>Follow the steps below to deploy the Kepler exporter as a Daemonset to run on all Nodes. The following deployment will also create a service listening on port <code>9102</code>.</p> <p>First, fork the kepler repository and clone it.</p> <p>Then, build the manifests file that suit your environment and deploy it with the following steps:</p>"},{"location":"installation/kepler/#build-manifests","title":"Build manifests","text":"<pre><code>make build-manifest OPTS=\"&lt;deployment options&gt;\"\n# minimum deployment: \n# &gt; make build-manifest\n# deployment with sidecar on openshift: \n# &gt; make build-manifest OPTS=\"ESTIMATOR_SIDECAR_DEPLOY OPENSHIFT_DEPLOY\"\n</code></pre> <p>Manifests will be generated in  <code>_output/manifests/kubernetes/generated/</code> by default.</p> Deployment Option Description Dependency BM_DEPLOY baremetal deployment patched with node selector feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR to not exist - OPENSHIFT_DEPLOY patch openshift-specific attribute to kepler daemonset and deploy SecurityContextConstraints - PROMETHEUS_DEPLOY patch prometheus-related resource (ServiceMonitor, RBAC role, rolebinding) require prometheus deployment which can be OpenShift integrated or custom deploy CLUSTER_PREREQ_DEPLOY deploy prerequisites for kepler on openshift cluster OPENSHIFT_DEPLOY option set CI_DEPLOY update proc path for kind cluster using in CI - ESTIMATOR_SIDECAR_DEPLOY patch estimator sidecar and corresponding configmap to kepler daemonset - MODEL_SERVER_DEPLOY deploy model server and corresponding configmap to kepler daemonset - TRAIN_DEPLOY patch online-trainer sidecar to model server MODEL_SERVER_DEPLOY option set -  build-manifest requirements: -  kubectl v1.21+ -  make -  go -  manifest sources and outputs will be in  <code>_output/generated-manifest</code> by default"},{"location":"installation/kepler/#deploy-using-kubectl","title":"Deploy using Kubectl","text":"<pre><code># kubectl apply -f _output/generated-manifest/deployment.yaml\n</code></pre>"},{"location":"installation/kepler/#deploy-the-prometheus-operator-and-the-whole-monitoring-stack","title":"Deploy the Prometheus operator and the whole monitoring stack","text":"<p>If Prometheus is already installed in the cluster, skip this step. Otherwise, follow these steps to install it.</p> <ol> <li>Clone the kube-prometheus project to your local folder.</li> </ol> <pre><code>git clone https://github.com/prometheus-operator/kube-prometheus\n</code></pre> <ol> <li>Deploy the whole monitoring stack using the config in the <code>manifests</code> directory. Create the namespace and CRDs, and then wait for them to be available before creating the remaining resources. During the <code>until</code> loop, a response of <code>No resources found</code> is to be expected. This statement checks whether the resource API is created but doesn't expect the resources to be there.</li> </ol> <pre><code>cd kube-prometheus\nkubectl apply --server-side -f manifests/setup\nuntil kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done\nkubectl apply -f manifests/\n</code></pre>"},{"location":"model_training/model_profile/","title":"Model Profile","text":"<p>To form a group of machines (nodes), the idea is to run a benchmark suite towards a bunch of machines, collect performance reported by the benchmarks, and apply clustering algorithm such as kmeans.</p> <p>For each group (<code>node_type</code>), we make a profile composing of background power when the resource usage is almost constant without user workload, minimum, maximum power for each power components (e.g., core, uncore, dram, package, platform), and normalization scaler (i.e., MinMaxScaler), standardization scaler (i.e., StandardScaler) for each feature group.</p> <p>Check the profiling tool here.</p>"},{"location":"model_training/pipeline/","title":"Training Pipeline","text":"<p>Kepler forms multiple groups of machines (nodes) based on its benchmark performance and trains a model separately for each group. The identified group is exported as <code>node_type</code>. </p> <p>For each <code>node_type</code>, there are two kinds of power model (output), AbsPower and DynPower, being trained with different groups of features (input). </p>"},{"location":"model_training/pipeline/#model-output-type","title":"Model Output Type","text":"<p>AbsPower: the predicted power is a node power that includes nearly-static power at idling state and dynamic power when running the process on the machine.</p> <p>DynPower: the predicted power is per-process/per-container power that excludes nearly-static power at idling state.</p>"},{"location":"model_training/pipeline/#model-features","title":"Model Features","text":"<p>The features are groupped by the metric sources.</p> <p>Feature group:</p> Group Name Features Metric Source(s) CounterOnly COUNTER_FEAUTRES Hardware Counter CgroupOnly CGROUP_FEATURES cGroups BPFOnly BPF_FEATURES BPF KubeletOnly KUBELET_FEATURES Kubelet IRQOnly IRQ_FEATURES BPF CounterIRQCombined COUNTER_FEAUTRES, IRQ_FEATURES BPF and Hardware Counter WorkloadOnly COUNTER_FEAUTRES, CGROUP_FEATURES, BPF_FEATURES, IRQ_FEATURES, KUBELET_FEATURES All except node information Full WORKLOAD_FEATURES, SYSTEM_FEATURES All"},{"location":"model_training/pipeline/#pipeline","title":"Pipeline","text":"<p>A training pipeline starts from reading the Kepler-exporting metrics from Prometheus query (prom) and finally submits an archived models to the model database (model DB). The pipeline is generally composed of one extractor, one isolator, and multiple trainers. Extractor extracts data from the query result for each feature group attached with measured power of each components (e.g., core, dram, package, uncore, platform). Isolator further removed the nearly-static power at idling state which is unrelated to the process/container from the extracted results. The output from isolator is used by DynPower trainers while AbsPower trainers directly use the output from extractor.</p> <p>In the current implementation, there are two default active pipelines as shown below. </p> <p></p>"},{"location":"model_training/pipeline/#extractor","title":"Extractor","text":"<p>From Kepler queries, the default extractor generates a dataframe with the following columns.</p> timestamp features labels[unit]_[#unit]_[component]_power node type timestamp e.g.,cgroupfs_cpu_usage_uscgroupfs_memory_usage_bytescgroupfs_system_cpu_usage_uscgroupfs_user_cpu_usage_us e.g.,package_0_package_powerpackage_1_package_powerpackage_0_core_powerpackage_1_core_powerpackage_0_uncore_powerpackage_1_uncore_powerpackage_0_dram_powerpackage_1_dram_power node_type"},{"location":"model_training/pipeline/#isolator","title":"Isolator","text":"<p>There are two available isolators: ProfileIsolator and MinIdleIsolator. </p> <p>ProfileIsolator relies on profiled background powers (profiles) and removes resource usages by system processes from the training while MinIdleIsolator assumes minimum power as an idle power and includes resource usages by system processes in the training. </p> <p>The pipeline with ProfileIsolator will be applied first if the profile that matches the training <code>node_type</code> is available. Otherwise, the other pipeline will be applied. </p> <p>(check how profiles are generated here)</p>"},{"location":"model_training/pipeline/#trainer","title":"Trainer","text":"<p>Trainers implements <code>Trainer</code> class (with 9 abstract methods). The common process for each <code>node_type</code> is to </p> <ol> <li> <p>load previous checkpoint model via implemented <code>(i) load_local_checkpoint</code> or <code>(ii) load_remote_checkpoint</code>. If the checkpoint cannot be loaded, initialize the model by calling implemented <code>(iii) init_model</code>.</p> </li> <li> <p>load and apply scaler to input data</p> </li> <li> <p>call implemented <code>(iv) train</code> and save the checkpoint via <code>(v) save_checkpoint</code></p> </li> <li> <p>check whether to achive the model and push to database via <code>(vi) should_archive</code>. If yes, </p> <p>4.1.  get triner-specific basic metdata via <code>(vii) get_basic_metadata</code></p> <p>4.2. fill with required metadata, save it as metadata file (metadata.json)</p> <p>4.3. call <code>(viii) save_model</code></p> <p>4.4. If <code>(iv) get_weight_dict</code> function is implemented (only for linear regression based trainer), the weight dict will be saved in the file named <code>weight.json</code>.</p> <p>4.5. archive the model folder. The model name will be in the format <code>&lt;trainer class&gt;_&lt;node_type&gt;</code>.</p> <p>4.6. push the archived model and <code>weight.json</code> (if available) to the database</p> </li> </ol> <p>If the trainer is based on scikit-learn, consider implementing only <code>init_model</code> method of <code>ScikitTrainer</code>.</p> <p>The intermediate checkpoint and output of model will be saved locally in folder <code>MODEL_PATH/&lt;PowerSource&gt;/&lt;ModelOutputType&gt;/&lt;FeatureGroup&gt;</code>. The default <code>MODEL_PATH</code> is <code>src/models</code>.</p>"},{"location":"project/contributing/","title":"Contributing","text":"<p>We welcome all kinds of contributions to Kepler from the community!</p> <p>For an in-depth guide on how to get started, checkout the Contributing Guide here.</p>"},{"location":"project/resources/","title":"Resources","text":""},{"location":"project/resources/#talks-demos","title":"Talks &amp; Demos","text":"<p>We really appreciate talks and demos about Kepler from the community. If you have made a presentation that demonstrated or referenced Kepler, please open a PR to add it to this page!</p>"},{"location":"project/resources/#kepler-demos","title":"Kepler Demos","text":"<p>The list below contains talks that demo Kepler, its capabilities, and how to gather energy metrics of various Kubernetes resources.</p> <ul> <li> <p>\"Sustainability the Container Native Way\", Huamin Chen (Red Hat) &amp; Chen Wang (IBM), Open Source Summit NA 2022 [slides]</p> </li> <li> <p>\"Sustainability Research the Cloud Native Way\", Chen Wang (IBM) &amp; Huamin Chen (Red Hat), KubeCon NA 2022 [slides]</p> </li> <li> <p>\"Sustainability in Computing: Energy Efficient Placements of Edge Workloads\", Parul Singh &amp; Kaiyi Liu (Red Hat), Kubernetes Edge Day NA 2022 [slides]</p> </li> <li> <p>\"Green(ing) CI/CD: A Sustainability Journey with GitOps\", Niki Manoledaki (Weaveworks), GitOpsCon NA 2022 [slides]</p> </li> </ul>"},{"location":"project/resources/#kepler-references","title":"Kepler References","text":"<p>The list below contains talks that reference Kepler in discussions on energy efficiency and cloud-native environmental sustainability.</p> <ul> <li> <p>\"Panel Discussion: Moving Towards Environmentally Sustainable Operations with Cloud Native Tools\", Niki Manoledaki (Weaveworks), Chris Lavery (Weaveworks), Marlow Weston (Intel), William Caban (Red Hat) [recording]</p> </li> <li> <p>\"How to Get Involved in CNCF Environmental Sustainability TAG\", Marlow Weston (Intel) &amp; Huamin Chen (Red Hat), KubeCon NA 22 [recording]</p> </li> <li> <p>\"Smart Green Computing Cloud Native Operations\", William Caban &amp; Federico Rossi (Red Hat), KubeCon NA 2022 [recording]</p> </li> </ul>"},{"location":"project/support/","title":"Support","text":""},{"location":"project/support/#the-best-ways-to-seek-support-are","title":"The best ways to seek support are","text":"<ol> <li> <p>Opening an issue in Kepler.</p> </li> <li> <p>Starting a discussion</p> </li> </ol>"},{"location":"usage/general_config/","title":"Configuration","text":"<p>This is a list of configurable values of Kepler System. The configuration can be also applied by defining the following CR spec if Kepler Operator is installed.</p> Point of Configuration Spec Description Default Kepler CR (single item: default) Kepler DaemonSet Deployment daemon.exporter.image Kepler main image quay.io/sustainable_computing_io/kepler:latest Kepler DaemonSet Deployment daemon.exporter.port Metric exporter port 9102 Kepler DaemonSet Deployment daemon.estimator-sidecar.enabled Kepler Estimator Sidecar patch false Kepler DaemonSet Deployment daemon.estimator-sidecar.image Kepler estimator sidecar image quay.io/sustainable_computing_io/kepler-estimator:latest Kepler DaemonSet Deployment daemon.estimator-sidecar.mnt-path Mount path between main container and the sidecar for unix domain socket /tmp Kepler DaemonSet Enviroment (METRIC_PATH) daemon.exporter.path Path to export metrics /metrics Kepler DaemonSet Enviroment (MODEL_SERVER_ENABLE) model-server.enaled Kepler Model Server Pod Pod connection false [model-server.enaled] Model Server Pod Pod Environment (MODEL_SERVER_PORT) model-server.port Model serving port of model server 8100 Model Server Pod Pod Environment (PROM_SERVER) model-server.prom Endpoint to Prometheus metric server http://prometheus-k8s.monitoring.svc.cluster.local:9090 Model Server Pod Pod Environment (MODEL_PATH) model-server.model-path Path to keep models models Kepler DaemonSet Environment (MODEL_SERVER_ENDPOINT) daemon.model-server Endpoint to server container of model server http://kepler-model-server.monitoring.cluster.local:[model-server.port]/model Model Server Pod Deployment model-server.trainer Model online trainer patch false [model-server.trainer] Model Server Pod Environment (PROM_QUERY_INTERVAL) model-server.prom_interval Interval to execute trainning pipelines in seconds 20 Model Server Pod Environment (PROM_QUERY_STEP) model-server.prom-step Step of query data point in trainning pipelines in seconds 3 Model Server Pod Environment (PROM_HEADERS) model-server.prom-header For specify required header (such as authentication) - Model Server Pod Environment (PROM_SSL_DISABLE) model-server.prom-ssl Disable ssl in Prometheus connection true Model Server Pod Environment (INITIAL_MODELS_LOC) model-server.init-loc Root URL of offline models to use as initial models https://raw.githubusercontent.com/sustainable-computing-io/kepler-model-server/main/tests/test_models Model Server Pod Environment (INITIAL_MODEL_NAMES.[MODEL_TYPE]) model-server.[MODEL_TYPE] Name of default pipeline for each model type - CollectMetric CR (single item: default) Kepler DaemonSet Environment (COUNTER_METRICS) counter List of performance metrics to enable from counter source * (enable all available metrics from counter source) Kepler DaemonSet Environment (CGROUP_METRICS) cgroup List of performance metrics to enable from cgroup source * (enable all available metrics from cgroup source) Kepler DaemonSet Environment (BPF_METRICS) bpf List of performance metrics to enable from bpf (aka. ebpf) source * (enable all available metrics from bpf source) Kepler DaemonSet Environment (KUBELET_METRICS) kubelet List of performance metrics to enable from kubelet source * (enable all available metrics from kubelet source) Kepler DaemonSet Environment (GPU_METRICS) gpu List of performance metrics to enable from gpu source * (enable all available metrics from gpu source) ExportMetric CR (single item: default) Kepler DaemonSet Environment (PERF_METRICS) perf List of performance metrics to export * (enable all collected performance metrics) Kepler DaemonSet Environment (EXPORT_NODE_TOTAL_POWER) node_total_power Trigger whether to export node total power true Kepler DaemonSet Environment (EXPORT_NODE_COMPONENT_POWERS) node_component_powers Trigger whether to export node powers by components true Kepler DaemonSet Environment (EXPORT_POD_TOTAL_POWER) pod_total_power Trigger whether to export pod total power true Kepler DaemonSet Environment (EXPORT_POD_COMPONENT_POWERS) pod_component_powers Trigger whether to export pod powers by components true EstimatorConfig CR (multiple items: node-total-power, node-component-powers, pod-total-power, pod-component-powers) Kepler DaemonSet Environment (MODEL_CONFIG.[MODEL_ITEM]_ESTIMATOR) use-sidecar Triggle whether to use estimator sidecar for power estimation false Kepler DaemonSet Environment (MODEL_CONFIG.[MODEL_ITEM]_MODEL) fixed-model Specify model name (auto-selected) Kepler DaemonSet Environment (MODEL_CONFIG.[MODEL_ITEM]_FILTERS) filters Specify model filter conditions in string (auto-selected) Kepler DaemonSet Environment (MODEL_CONFIG.[MODEL_ITEM]_INIT_URL) init-url URL to initial model location - RatioConfig CR (single items: default) Kepler DaemonSet Environment (CORE_USAGE_METRIC) core_metric Specify metric for compute core (mostly cpu) component of pod power by ratio modeling cpu_instr Kepler DaemonSet Environment (DRAM_USAGE_METRIC) dram_metric Specify metric for computing dram component of pod power by ratio modeling cache_miss Kepler DaemonSet Environment (UNCORE_USAGE_METRIC) uncore_metric Specify metric for computing uncore component of pod power by ratio modeling (evenly divided) Kepler DaemonSet Environment (GENERAL_USAGE_METRIC) general_metric Specify metric for computing uncategorized component (pkg-core-uncore) of pod power by ratio modeling cpu_instr Kepler DaemonSet Environment (GPU_USAGE_METRIC) core_metric Specify metric for computing gpu component of pod power by ratio modeling - <p>Remarks:</p> <ul> <li>[MODEL_ITEM] can be either of the following values corresponding to item names: NODE_TOTAL, NODE_COMPONENT, POD_TOTAL, POD_COMPONENTS.</li> <li> <p>[MODEL_TYPE] is a concatenation of [MODEL_ITEM] and [OUTPUT_FORMAT] which can be either POWER for archived model or MODEL_WEIGHT for weights in json. </p> <p>For example,</p> <ul> <li>NODE_TOTAL_POWER: archived model to estimate node total power used by estimator sidecar</li> <li>POD_COMPONENTS_MODEL_WEIGHT: model weight to estimate pod component powers used by linear regressor embbed in Kepler main component.</li> </ul> </li> </ul>"},{"location":"usage/kepler_daemon/","title":"Kepler DaemonSet Customization","text":"<p>Kepler enables a function to hybrid read environment variable from attributes directly (container.env) and from the ConfigMap. Note that, all steps will be operated by Kepler Operator if the operator is installed.</p> <p>To set environments by ConfigMap:</p> <ol> <li>Create/Generate ConfigMap</li> </ol> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kepler-cfm\n  namespace: kepler-system\ndata:\n  MODEL_SERVER_ENABLE: true\n  COUNTER_METRICS: '*'\n  CGROUP_METRICS: '*'\n  BPF_METRICS: '*'\n  # KUBELET_METRICS: ''\n  # GPU_METRICS: ''\n  PERF_METRICS: '*'\n  MODEL_CONFIG: |\n    POD_COMPONENT_ESTIMATOR=true\n    POD_COMPONENT_INIT_URL=https://raw.githubusercontent.com/sustainable-computing-io/kepler-model-server/main/tests/test_models/DynComponentPower/CgroupOnly/ScikitMixed.zip\n</code></pre> <ol> <li>Mount the confimap to DeamonSet:</li> </ol> <pre><code>  spec:\n    containers:\n      - name: kepler-exporter\n        volumeMounts:\n        - name: cfm\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: cfm\n        configMap:\n          name: kepler-cfm\n</code></pre>"},{"location":"usage/model_server/","title":"Model Server Customization","text":"<p>Note that, all steps will be operated by Kepler Operator if the operator is installed.</p> <p>To set environments by ConfigMap,</p> <ol> <li> <p>Create/Generate ConfigMap</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kepler-model-server-cfm\n  namespace: kepler-system\ndata:\n  MODEL_PATH: models\n  PROM_SERVER: 'http://prometheus-k8s.monitoring.svc.cluster.local:9090'\n  PROM_QUERY_INTERVAL: '20'\n  PROM_QUERY_STEP: '3'\n  PROM_HEADERS: ''\n  PROM_SSL_DISABLE: 'true'\n  INITIAL_MODELS_LOC: https://raw.githubusercontent.com/sustainable-computing-io/kepler-model-server/main/tests/test_models\n</code></pre> </li> <li> <p>Mount the confimap to DeamonSet</p> <pre><code>spec:\n  containers:\n    - name: server-api\n      volumeMounts:\n      - name: cfm\n        mountPath: /etc/config\n        readOnly: true\n    volumes:\n    - name: cfm\n      configMap:\n        name: kepler-model-server-cfm\n</code></pre> </li> </ol>"},{"location":"usage/trouble_shooting/","title":"Trouble Shooting","text":""},{"location":"usage/trouble_shooting/#kepler-pod-failed-to-start","title":"Kepler Pod failed to start","text":""},{"location":"usage/trouble_shooting/#background","title":"Background","text":"<p>Kepler uses eBPF to obtain performance counter readings and processes stats. Since eBPF requires kernel headers, Kepler will fail to start up when the kernel headers are missing.</p>"},{"location":"usage/trouble_shooting/#diagnose","title":"Diagnose","text":"<p>To confirm, check the Kepler Pod logs with the following command and look for message <code>not able to load eBPF modules</code>. </p> <pre><code>kubectl logs -n kepler daemonset/kepler-exporter\n</code></pre>"},{"location":"usage/trouble_shooting/#solution","title":"Solution","text":"<p>Installing kernel headers on each node can be done manually using the following command</p> <pre><code># Fedora/RHEL based distro\ndnf install kernel-devel-`uname -r` -y\n# Debian/Ubuntu distro\napt install linux-headers-$(uname -r)\n</code></pre> <p>On OpenShift, install the MachineConfiguration here</p>"},{"location":"usage/trouble_shooting/#kepler-energy-metrics-are-zeroes","title":"Kepler energy metrics are zeroes","text":""},{"location":"usage/trouble_shooting/#background_1","title":"Background","text":"<p>Kepler uses RAPL counters on x86 platforms to read energy consumption.  VMs do not have RAPL counters and thus Kepler estimates energy consumption based on the pre-trained ML models. The models use either hardware performance counters or cGroup stats to estimate energy consumed by processes. Currently the cGroup based models use cGroup v2 features such as <code>cgroupfs_cpu_usage_us</code>, <code>cgroupfs_memory_usage_bytes</code>, <code>cgroupfs_system_cpu_usage_us</code>, <code>cgroupfs_user_cpu_usage_us</code>, <code>bytes_read</code>, and <code>bytes_writes</code>.</p>"},{"location":"usage/trouble_shooting/#diagnose_1","title":"Diagnose","text":"<p>The Kepler metrics are zeroes, check if cGroup version on the node:</p> <pre><code>ls /sys/fs/cgroup/cgroup.controllers\n</code></pre>"},{"location":"usage/trouble_shooting/#solution_1","title":"Solution","text":"<p>Enable cGroup v2 on the node by following these Kubernetes instruction.</p> <p>On OpenShift, apply these cGroup v2 MachineConfiguration</p>"}]}